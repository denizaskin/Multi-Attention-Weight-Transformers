# Quick Reference: Metrics for Paper Reporting

## üìä What to Report in Main Paper

### **Main Results Table (Required):**

```
Table 1: Performance on IR Benchmarks (Test Set)

Dataset    | Model      | NDCG@10 | MRR@10 | MAP   | P@10  | R@100
-----------|------------|---------|--------|-------|-------|-------
MS MARCO   | Non-MAW    | 0.880   | 1.000  | 0.666 | 0.500 | 1.000
           | MAW+Super  | 0.815   | 1.000  | 0.493 | 0.300 | 1.000
           | MAW+GRPO   | 0.832   | 1.000  | 1.000 | 0.500 | 1.000
TREC-DL    | Non-MAW    | 0.xxx   | x.xxx  | x.xxx | x.xxx | x.xxx
           | MAW+Super  | 0.xxx   | x.xxx  | x.xxx | x.xxx | x.xxx
           | MAW+GRPO   | 0.xxx   | x.xxx  | x.xxx | x.xxx | x.xxx
...
```

**Caption:**
> Comparison of Multi-Attention-Weight (MAW) Transformers with supervised classification and GRPO reinforcement learning against Non-MAW baseline. Primary metrics: NDCG@10 (position-sensitive ranking quality), MRR@10 (mean reciprocal rank), and MAP (mean average precision). P@10 = Precision@10, R@100 = Recall@100. Bold indicates best performance. All models evaluated on identical held-out test sets (20% of data).

---

## üìà What to Report in Supplementary Material

### **Full K-Value Breakdown:**

```
Table S1: Complete Metric Breakdown for MS MARCO

Metric     | Model      | K=1   | K=5   | K=10  | K=20  | K=100 | K=1000
-----------|------------|-------|-------|-------|-------|-------|-------
NDCG       | Non-MAW    | 1.000 | 0.919 | 0.880 | 0.894 | 0.919 | 0.919
           | MAW+Super  | 1.000 | 0.871 | 0.815 | 0.878 | 0.916 | 0.916
           | MAW+GRPO   | 0.670 | 0.916 | 0.832 | 0.815 | 0.903 | 0.903
Precision  | Non-MAW    | 1.000 | 0.800 | 0.500 | 0.300 | 0.160 | 0.160
           | MAW+Super  | 1.000 | 0.600 | 0.300 | 0.250 | 0.160 | 0.160
           | MAW+GRPO   | 1.000 | 1.000 | 0.500 | 0.250 | 0.100 | 0.100
Recall     | Non-MAW    | 0.125 | 0.500 | 0.625 | 0.750 | 1.000 | 1.000
           | MAW+Super  | 0.125 | 0.375 | 0.375 | 0.625 | 1.000 | 1.000
           | MAW+GRPO   | 0.200 | 1.000 | 1.000 | 1.000 | 1.000 | 1.000
MRR        | Non-MAW    | 1.000 | 1.000 | 1.000 | 1.000 | 1.000 | 1.000
           | MAW+Super  | 1.000 | 1.000 | 1.000 | 1.000 | 1.000 | 1.000
           | MAW+GRPO   | 1.000 | 1.000 | 1.000 | 1.000 | 1.000 | 1.000
MAP        | Non-MAW    | 0.666 | -     | -     | -     | -     | -
           | MAW+Super  | 0.493 | -     | -     | -     | -     | -
           | MAW+GRPO   | 1.000 | -     | -     | -     | -     | -

Note: MAP is computed over full ranking (no K cutoff).
```

---

## üìù Methods Section - Evaluation Metrics

### **Text for Paper:**

> **Evaluation Metrics.** We evaluate our models using standard information retrieval metrics at multiple cutoff values K ‚àà {1, 5, 10, 20, 100, 1000}. Following established benchmarks (MS MARCO, TREC-DL), we report:
>
> **NDCG@K** (Normalized Discounted Cumulative Gain) [J√§rvelin & Kek√§l√§inen, 2002], which measures position-sensitive ranking quality with graded relevance:
> ```
> NDCG@K = DCG@K / IDCG@K
> where DCG@K = Œ£(i=1 to K) (2^rel_i - 1) / log‚ÇÇ(i + 1)
> ```
>
> **MRR@K** (Mean Reciprocal Rank) [Voorhees, 1999], the reciprocal rank of the first relevant document:
> ```
> MRR@K = 1/Q Œ£(q=1 to Q) 1/rank_q
> ```
>
> **MAP** (Mean Average Precision) [Buckley & Voorhees, 2000], the mean of average precision across all queries, computed over the full ranking:
> ```
> MAP = 1/Q Œ£(q=1 to Q) AP_q
> where AP_q = 1/|R_q| Œ£(k=1 to n) P@k √ó rel(k)
> ```
>
> **Precision@K**, the fraction of top-K documents that are relevant:
> ```
> P@K = |relevant ‚à© top-K| / K
> ```
>
> **Recall@K**, the fraction of relevant documents retrieved in top-K:
> ```
> R@K = |relevant ‚à© top-K| / |relevant|
> ```
>
> We report NDCG@10 and MRR@10 as primary metrics following MS MARCO and TREC-DL standards. MAP provides a single-value assessment of overall ranking quality. Full results across all K values are provided in supplementary materials.

---

## üéØ Interpretation Guide for Reviewers

### **High NDCG@10 (> 0.8):**
- Model ranks relevant documents near top
- Position-sensitive: earlier results weighted more
- Gold standard metric in IR

### **High MRR (> 0.8):**
- First relevant result appears early
- Important for user satisfaction
- Standard in web search evaluation

### **High MAP (> 0.7):**
- Overall ranking quality is excellent
- Considers all relevant documents
- No cutoff bias

### **Precision vs Recall Trade-off:**
- High P@10, Low R@10: Conservative, high accuracy
- Low P@100, High R@100: Comprehensive, high coverage
- Optimal: Balance at K=10 or K=20

---

## üìä Comparison to Baselines

### **If you implement architectural baselines:**

```
Table 2: Ablation Study - Depth Selection Mechanisms

Method              | NDCG@10 | MRR@10 | MAP   | Description
--------------------|---------|--------|-------|---------------------------
Non-MAW             | 0.880   | 1.000  | 0.666 | Standard Transformer (no depth routing)
Random Depth        | 0.xxx   | x.xxx  | x.xxx | Random depth selection
Fixed Depth (16)    | 0.xxx   | x.xxx  | x.xxx | Single depth dimension
32 Heads (no MAW)   | 0.xxx   | x.xxx  | x.xxx | More heads instead of depth
MAW+Supervised      | 0.815   | 1.000  | 0.493 | Neural network depth router
MAW+GRPO (ours)     | 0.832   | 1.000  | 1.000 | RL-based depth router ‚≠ê
```

---

## üìö Key Citations

**Metrics:**
- NDCG: J√§rvelin, K. & Kek√§l√§inen, J. (2002). "Cumulated gain-based evaluation of IR techniques." ACM TOIS.
- MRR: Voorhees, E. M. (1999). "The TREC-8 Question Answering Track Report." TREC.
- MAP: Buckley, C. & Voorhees, E. M. (2000). "Evaluating evaluation measure stability." SIGIR.

**Benchmarks:**
- MS MARCO: Nguyen, T. et al. (2016). "MS MARCO: A human generated machine reading comprehension dataset." NIPS.
- TREC-DL: Craswell, N. et al. (2019-2021). "Overview of the TREC 2019-2021 deep learning track."
- Natural Questions: Kwiatkowski, T. et al. (2019). "Natural Questions: A benchmark for question answering research." TACL.

---

## ‚úÖ Checklist Before Submission

### **Main Paper:**
- [ ] Table 1: Main results with NDCG@10, MRR@10, MAP, P@10, R@100
- [ ] Methods section describing all metrics
- [ ] Proper citations for metrics
- [ ] Interpretation of results
- [ ] Statistical significance tests (t-test or Wilcoxon)

### **Supplementary Material:**
- [ ] Table S1: Full K-value breakdown for all metrics
- [ ] Per-dataset detailed results
- [ ] Additional analysis (per-query, failure cases)
- [ ] Hyperparameter sensitivity

### **Code/Data:**
- [ ] Evaluation scripts available
- [ ] Results reproducible
- [ ] Clear documentation

---

## üöÄ Final Recommendation

### **For Strong Tier-1 Paper:**

**Main Results Table:** Show 4-5 key metrics
- NDCG@10 (primary - gold standard)
- MRR@10 (secondary - user-centric)
- MAP (overall quality)
- P@10 or R@100 (one complementary metric)

**Supplementary Material:** Full breakdown
- All metrics at all K values
- Per-dataset analysis
- Statistical tests

**Methods Section:**
- Define all metrics mathematically
- Cite original papers
- Justify K-value choices

**Your implementation already provides all of this!** ‚úÖ

---

## üí° Pro Tips

1. **Always report NDCG@10** - it's expected by reviewers
2. **Include MAP** - shows you care about full ranking quality
3. **Show multiple K values** - demonstrates robustness
4. **Report statistical significance** - p-values strengthen claims
5. **Compare to strong baselines** - not just random or vanilla

Your implementation now covers all of these! üéâ
